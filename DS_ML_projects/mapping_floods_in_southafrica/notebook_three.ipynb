{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model #type: ignore\n",
    "from tensorflow.keras.layers import Input, Dense, Conv1D, MaxPooling1D, Flatten, Dropout, GlobalAveragePooling2D, concatenate #type:ignore\n",
    "from tensorflow.keras.applications import ResNet50  #type: ignore\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import log_loss\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from functools import partial\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('Train.csv')\n",
    "test_data = pd.read_csv('Test.csv')\n",
    "composite_images = np.load('composite_images.npz')\n",
    "images_path = 'composite_images.npz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = np.load(images_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              event_id  precipitation  label       event_id_2  event_idx  \\\n",
      "0  id_spictby0jfsb_X_0       0.000000      0  id_spictby0jfsb          0   \n",
      "1  id_spictby0jfsb_X_1       0.095438      0  id_spictby0jfsb          1   \n",
      "2  id_spictby0jfsb_X_2       1.949560      0  id_spictby0jfsb          2   \n",
      "3  id_spictby0jfsb_X_3       3.232160      0  id_spictby0jfsb          3   \n",
      "4  id_spictby0jfsb_X_4       0.000000      0  id_spictby0jfsb          4   \n",
      "\n",
      "   event_t  \n",
      "0        0  \n",
      "1        0  \n",
      "2        0  \n",
      "3        0  \n",
      "4        0  \n",
      "              event_id  precipitation       event_id_2  event_idx  event_t\n",
      "0  id_j7b6sokflo4k_X_0        0.00000  id_j7b6sokflo4k          0        0\n",
      "1  id_j7b6sokflo4k_X_1        3.01864  id_j7b6sokflo4k          1        0\n",
      "2  id_j7b6sokflo4k_X_2        0.00000  id_j7b6sokflo4k          2        0\n",
      "3  id_j7b6sokflo4k_X_3       16.61520  id_j7b6sokflo4k          3        0\n",
      "4  id_j7b6sokflo4k_X_4        2.56706  id_j7b6sokflo4k          4        0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_data['event_id_2'] = train_data['event_id'].apply(lambda x: '_'.join(x.split('_')[0:2]))\n",
    "train_data['event_idx'] = train_data.groupby('event_id', sort=False).ngroup()\n",
    "test_data['event_id_2'] = test_data['event_id'].apply(lambda x: '_'.join(x.split('_')[0:2]))\n",
    "test_data['event_idx'] = test_data.groupby('event_id', sort=False).ngroup()\n",
    "\n",
    "train_data['event_t'] = train_data.groupby('event_id').cumcount()\n",
    "test_data['event_t'] = test_data.groupby('event_id').cumcount()\n",
    "\n",
    "print(train_data.head())\n",
    "print(test_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocessing functions\n",
    "_MAX_INT = np.iinfo(np.uint16).max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_slope(x: np.ndarray) -> np.ndarray:\n",
    "    # Convert 16-bit discretized slope to float32 radians\n",
    "    return (x / _MAX_INT * (math.pi / 2.0)).astype(np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(x: np.ndarray, mean: int, std: int) -> np.ndarray:\n",
    "    return (x - mean) / std\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "rough_S2_normalize = partial(normalize, mean=1250, std=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(x: np.ndarray) -> np.ndarray:\n",
    "    return np.concatenate([\n",
    "        rough_S2_normalize(x[..., :-1].astype(np.float32)),\n",
    "        decode_slope(x[..., -1:]),\n",
    "    ], axis=-1, dtype=np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "train_data['precipitation'] = scaler.fit_transform(train_data[['precipitation']])\n",
    "test_data['precipitation'] = scaler.transform(test_data[['precipitation']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare training and validation splits\n",
    "rng = np.random.default_rng(seed=0xf100d)\n",
    "event_ids = train_data['event_id'].unique()\n",
    "new_split = pd.Series(\n",
    "    data=rng.choice(['train', 'valid'], size=len(event_ids), p=[0.9, 0.1]),\n",
    "    index=event_ids,\n",
    "    name='split',\n",
    ")\n",
    "data_new = train_data.join(new_split, on='event_id')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = data_new[(data_new['split'] == 'train')]\n",
    "train_timeseries = train_df.pivot(index='event_id', columns='event_t', values='precipitation').to_numpy()\n",
    "train_labels = train_df.pivot(index='event_id', columns='event_t', values='label').to_numpy()\n",
    "\n",
    "valid_df = data_new[data_new['split'] == 'valid']\n",
    "valid_timeseries = valid_df.pivot(index='event_id', columns='event_t', values='precipitation').to_numpy()\n",
    "valid_labels = valid_df.pivot(index='event_id', columns='event_t', values='label').to_numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the test set there are no labels\n",
    "test_timeseries = test_data.pivot(index='event_id', columns='event_t', values='precipitation').to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images, valid_images, test_images = [], [], []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 674/674 [00:40<00:00, 16.53it/s]\n"
     ]
    }
   ],
   "source": [
    "# Process train and validation images\n",
    "for event_id in tqdm(data_new['event_id_2'].unique()):\n",
    "    img = preprocess_image(images[event_id])\n",
    "    if data_new[data_new['event_id_2'] == event_id]['split'].iloc[0] == 'train':\n",
    "        train_images.append(img)\n",
    "    else:\n",
    "        valid_images.append(img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/224 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 224/224 [00:01<00:00, 181.97it/s]\n"
     ]
    }
   ],
   "source": [
    "# Process test images\n",
    "for event_id in tqdm(test_data['event_id_2'].unique()):\n",
    "    img = preprocess_image(images[event_id])\n",
    "    test_images.append(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = np.stack(train_images, axis=0)\n",
    "valid_images = np.stack(valid_images, axis=0)\n",
    "test_images = np.stack(test_images, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_time_series_resnet(input_shape):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = Conv1D(64, kernel_size = 3, activation='selu', padding ='same')(inputs)\n",
    "    x = MaxPooling1D(pool_size=2)(x)\n",
    "    x = Conv1D(128, kernel_size=3, activation = 'selu', padding = 'same')(x)\n",
    "    x = MaxPooling1D(pool_size=2)(x)\n",
    "    x = Conv1D(256, kernel_size=3, activation='selu', padding='same')(x)\n",
    "    x = MaxPooling1D(pool_size=2)(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    outputs = Dense(64, activation='relu')(x)\n",
    "    return Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming the input shape should be (steps, input_dim)\n",
    "# For example, if you have 10 time steps and 1 feature, the input shape should be (10, 1)\n",
    "precipitation_model = create_time_series_resnet((train_timeseries.shape[1], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define image model (ResNet)\n",
    "image_input = Input(shape=(train_images.shape[1], train_images.shape[2], train_images.shape[3]))\n",
    "base_model = ResNet50(weights=None, include_top=False, input_tensor=image_input)\n",
    "image_features = GlobalAveragePooling2D()(base_model.output)\n",
    "image_model = Model(inputs=image_input, outputs=image_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine models\n",
    "combined_input_precipitation = Input(shape=(train_timeseries.shape[1], 1))\n",
    "combined_precipitation_features = precipitation_model(combined_input_precipitation)\n",
    "\n",
    "combined_input_image = Input(shape=(train_images.shape[1], train_images.shape[2], train_images.shape[3]))\n",
    "combined_image_features = image_model(combined_input_image)\n",
    "\n",
    "combined = concatenate([combined_precipitation_features, combined_image_features])\n",
    "x = Dense(128, activation=\"relu\")(combined)\n",
    "x = Dropout(0.3)(x)\n",
    "output = Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "model = Model(inputs=[combined_input_precipitation, combined_input_image], outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Data cardinality is ambiguous. Make sure all arrays contain the same number of samples.'x' sizes: 442716, 614\n'y' sizes: 442716\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 17\u001b[0m\n\u001b[0;32m     14\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m\"\u001b[39m, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary_crossentropy\u001b[39m\u001b[38;5;124m\"\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Train model\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m history \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(\n\u001b[0;32m     18\u001b[0m     [train_timeseries[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, np\u001b[38;5;241m.\u001b[39mnewaxis], train_images],\n\u001b[0;32m     19\u001b[0m     train_labels_reduced,\n\u001b[0;32m     20\u001b[0m     validation_data\u001b[38;5;241m=\u001b[39m([valid_timeseries[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, np\u001b[38;5;241m.\u001b[39mnewaxis], valid_images], valid_labels_reduced),\n\u001b[0;32m     21\u001b[0m     epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m,\n\u001b[0;32m     22\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m\n\u001b[0;32m     23\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Reinhard\\anaconda3\\envs\\Datascience\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\Reinhard\\anaconda3\\envs\\Datascience\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\data_adapter_utils.py:114\u001b[0m, in \u001b[0;36mcheck_data_cardinality\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m    110\u001b[0m     sizes \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\n\u001b[0;32m    111\u001b[0m         \u001b[38;5;28mstr\u001b[39m(i\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tree\u001b[38;5;241m.\u001b[39mflatten(single_data)\n\u001b[0;32m    112\u001b[0m     )\n\u001b[0;32m    113\u001b[0m     msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m sizes: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msizes\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 114\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n",
      "\u001b[1;31mValueError\u001b[0m: Data cardinality is ambiguous. Make sure all arrays contain the same number of samples.'x' sizes: 442716, 614\n'y' sizes: 442716\n"
     ]
    }
   ],
   "source": [
    "# Reduce target labels to match the model's output shape\n",
    "train_labels_reduced = np.max(train_labels, axis=1)\n",
    "valid_labels_reduced = np.max(valid_labels, axis=1)\n",
    "\n",
    "# Ensure the shapes of the input arrays match\n",
    "train_labels_reduced = train_labels_reduced[:train_timeseries.shape[0]]\n",
    "valid_labels_reduced = valid_labels_reduced[:valid_timeseries.shape[0]]\n",
    "\n",
    "# Ensure the shapes of the image arrays match\n",
    "train_images = train_images[:train_timeseries.shape[0]]\n",
    "valid_images = valid_images[:valid_timeseries.shape[0]]\n",
    "\n",
    "# Ensure the model is compiled before fitting\n",
    "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Train model\n",
    "history = model.fit(\n",
    "    [train_timeseries[..., np.newaxis], train_images],\n",
    "    train_labels_reduced,\n",
    "    validation_data=([valid_timeseries[..., np.newaxis], valid_images], valid_labels_reduced),\n",
    "    epochs=20,\n",
    "    batch_size=32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 729ms/step\n",
      "Log Loss: 1.6936372370331882\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model\n",
    "y_val_pred = model.predict([valid_timeseries[..., np.newaxis], valid_images])\n",
    "print(f\"Log Loss: {log_loss(valid_labels_reduced, y_val_pred)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1s/step\n"
     ]
    }
   ],
   "source": [
    "# Predict on test data\n",
    "y_test_pred = model.predict([test_timeseries[..., np.newaxis], test_images])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the lengths match\n",
    "\n",
    "# Save predictions for submission\n",
    "submission = pd.DataFrame({\n",
    "    \"event_id\": test_data[\"event_id\"].unique(),\n",
    "    \"label\": y_test_pred.flatten()[:len(test_data[\"event_id\"])]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv(\"submission_300.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Datascience",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
