{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model ## type: ignore\n",
    "from tensorflow.keras.layers import (Input, Dense, Conv1D, MaxPooling1D, Flatten, GlobalAveragePooling2D, Dropout, concatenate) ## type: ignore \n",
    "from tensorflow.keras.applications import ResNet50 ## type: ignore\n",
    "from sklearn.model_selection import train_test_split ## type: ignore\n",
    "from sklearn.preprocessing import MinMaxScaler ## type: ignore\n",
    "from sklearn.metrics import log_loss ## type: ignore\n",
    "import matplotlib as plt\n",
    "import math\n",
    "from functools import partial ## type: ignore\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('Train.csv')\n",
    "test_data = pd.read_csv('Test.csv')\n",
    "images_path = 'composite_images.npz'\n",
    "images = np.load(images_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "train_data['precipitation'] = scaler.fit_transform(train_data[['precipitation']])\n",
    "test_data['precipitation'] = scaler.transform(test_data[['precipitation']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_data['event_id'] = train_data['event_id'].apply(lambda x: '_'.join(x.split('_')[0:2]))\n",
    "train_data['event_idx'] = train_data.groupby('event_id', sort=False).ngroup()\n",
    "test_data['event_id'] = test_data['event_id'].apply(lambda x: '_'.join(x.split('_')[0:2]))\n",
    "test_data['event_idx'] = test_data.groupby('event_id', sort=False).ngroup()\n",
    "\n",
    "train_data['event_t'] = train_data.groupby('event_id').cumcount()\n",
    "test_data['event_t'] = test_data.groupby('event_id').cumcount()\n",
    "\n",
    "print(train_data.head())\n",
    "print(test_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoding the image metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BAND_NAMES = ('B2', 'B3', 'B4', 'B8', 'B11', 'slope')\n",
    "H, W, NUM_CHANNELS = IMG_DIM = (128, 128, len(BAND_NAMES))\n",
    "_MAX_INT = np.iinfo(np.int16).max\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_slope(X: np.ndarray) -> np.ndarray:\n",
    "    return (X / _MAX_INT * (math.pi / 2.0)).astype(np.float32)\n",
    "\n",
    "def normalize(x: np.ndarray, mean: int, std: int) -> np.ndarray:\n",
    "    return (x - mean) / std\n",
    "\n",
    "rough_S2_normalize = partial(normalize, mean=1250, std=500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(x: np.ndarray) -> np.ndarray:\n",
    "    return np.concatenate([\n",
    "        rough_S2_normalize(x[..., :-1].astype(np.float32)),\n",
    "        decode_slope(x[..., -1:]),\n",
    "    ], axis=-1, dtype=np.float32)\n",
    "\n",
    "images_path = 'composite_images.npz'\n",
    "composite_images = np.load(images_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data_and_images(data_df, composite_images):\n",
    "    event_ids = data_df['event_id'].unique()\n",
    "    timeseries = []\n",
    "    labels = []\n",
    "    images = []\n",
    "\n",
    "    for event_id in tqdm(event_ids, desc=\"Processing data\"):\n",
    "        event_data = data_df[data_df['event_id'] == event_id]\n",
    "        timeseries.append(event_data['precipitation'].values)\n",
    "        if 'label' in event_data.columns:\n",
    "            labels.append(event_data['label'].values)\n",
    "        images.append(preprocess_image(composite_images[event_id]))\n",
    "\n",
    "    timeseries = np.array(timeseries)\n",
    "    labels = np.array(labels) if labels else None\n",
    "    images = np.stack(images, axis=0)\n",
    "\n",
    "    return timeseries, labels, images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_timeseries, train_labels, train_images = preprocess_data_and_images(train_data, composite_images)\n",
    "test_timeseries, _, test_images = preprocess_data_and_images(test_data, composite_images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_timeseries.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the data into training and validation splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-validation split\n",
    "train_split, val_split = train_test_split(\n",
    "    np.arange(len(train_timeseries)), test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "X_precip_train, X_precip_val = train_timeseries[train_split], train_timeseries[val_split]\n",
    "y_train, y_val = train_labels[train_split], train_labels[val_split]\n",
    "X_img_train, X_img_val = train_images[train_split], train_images[val_split]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building our model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining our model for processing the timeseries data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Lambda #type: ignore    \n",
    "\n",
    "# Define precipitation model (1D ResNet)\n",
    "def create_time_series_resnet(input_shape):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    reshaped_inputs = Lambda(lambda x: tf.expand_dims(x, axis=-1))(inputs)  # Add an extra dimension\n",
    "    x = Conv1D(64, kernel_size=3, activation=\"relu\", padding=\"same\")(reshaped_inputs)\n",
    "    x = MaxPooling1D(pool_size=2)(x)\n",
    "    x = Conv1D(128, kernel_size=3, activation=\"relu\", padding=\"same\")(x)\n",
    "    x = MaxPooling1D(pool_size=2)(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    outputs = Dense(64, activation=\"relu\")(x)\n",
    "    return Model(inputs, outputs)\n",
    "\n",
    "precipitation_model = create_time_series_resnet((730,))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining our model for processing the image data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define image model (ResNet)\n",
    "image_input = Input(shape=(128, 128, 6))\n",
    "base_model = ResNet50(weights=None, include_top=False, input_tensor=image_input)\n",
    "image_features = GlobalAveragePooling2D()(base_model.output)\n",
    "image_model = Model(inputs=image_input, outputs=image_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### combining our models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine models\n",
    "combined_precip_input = Input(shape=(730,))\n",
    "combined_precip_features = precipitation_model(combined_precip_input)\n",
    "\n",
    "combined_image_input = Input(shape=(128, 128, 6))\n",
    "combined_image_features = image_model(combined_image_input)\n",
    "\n",
    "combined = concatenate([combined_precip_features, combined_image_features])\n",
    "x = Dense(128, activation=\"relu\")(combined)\n",
    "x = Dropout(0.3)(x)\n",
    "output = Dense(730, activation=\"sigmoid\")(x)\n",
    "output = Lambda(lambda x: tf.expand_dims(x, axis=-1))(output)  # Reshape to (None, 730, 1)\n",
    "\n",
    "model = Model(inputs=[combined_precip_input, combined_image_input], outputs=output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining our callbacks for model optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping #type: ignore\n",
    "\n",
    "# Create callbacks\n",
    "lr_scheduler = ReduceLROnPlateau(\n",
    "    monitor='val_loss',  # Reduce learning rate when validation loss plateaus\n",
    "    factor=0.5,          # Reduce learning rate by a factor of 0.5\n",
    "    patience=5,          # Wait for 5 epochs before reducing the learning rate\n",
    "    min_lr=1e-6,         # Minimum learning rate\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',  # Stop training when validation loss stops improving\n",
    "    patience=10,         # Wait for 10 epochs before stopping\n",
    "    restore_best_weights=True,  # Restore the best model weights\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape train_labels and val_labels to match the model's output shape\n",
    "y_train_reshaped = train_labels[train_split].reshape(-1, 730, 1)\n",
    "y_val_reshaped = train_labels[val_split].reshape(-1, 730, 1)\n",
    "\n",
    "# Ensure the model is compiled before training\n",
    "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Train model\n",
    "history = model.fit(\n",
    "    [X_precip_train, X_img_train],\n",
    "    y_train_reshaped,\n",
    "    validation_data=([X_precip_val, X_img_val], y_val_reshaped),\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    callbacks=[lr_scheduler, early_stopping],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating our model on the validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val_pred = model.predict([X_precip_val, X_img_val])\n",
    "print(f'Log loss: {log_loss(y_val_reshaped, y_val_pred)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating model performance using the training plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "from matplotlib import pyplot as plt    \n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making predictions on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = model.predict([test_timeseries, test_images]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Creating a submission file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.read_csv('SampleSubmission.csv')\n",
    "sub.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({\n",
    "    'event_id': sub['event_id'],\n",
    "    'label': y_test_pred.flatten()\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv(\"submission_final.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Datascience",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
